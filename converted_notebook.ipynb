{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9319fc1",
   "metadata": {},
   "source": [
    "In [1]:\n",
    "In [2]:\n",
    "In [3]:\n",
    "In [41]:\n",
    "In [5]:\n",
    "In [6]:\n",
    "In [7]:\n",
    "Hello! \n",
    "Welcome to NLP Learning. \n",
    "This is the day 1 of Webinar. \n",
    "We are trying to understand text basics. \n",
    "Lets move ahead. \n",
    "Out[6]:\n",
    "True\n",
    "# First Import spaCy and loading small version of the language library\n",
    "# After importing the spacy module we load a model and named it nlp.\n",
    "# Next we created a Doc object by applying the model to our text, and named it doc.\n",
    "# The Doc object holds the processed text.\n",
    "# spaCy also builds a companion Vocab object.\n",
    "# Import SpaCy and load model for core english language.\n",
    "import spacy\n",
    "# Create an object called nlp in which small english model is loaded.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#...............................Step 1: Sentence segmentation..............................\n",
    "# Demo of splitiing a paragraph into sentences\n",
    "doc1 = nlp(u'Hello! Welcome to NLP Learning. This is the day 1 of Webinar. We are trying to\n",
    "for sentence in doc1.sents:\n",
    "    print(sentence)\n",
    "doc1[0].is_sent_start\n",
    "doc1[1].is_sent_start\n",
    "\n",
    "\n",
    "In [8]:\n",
    "In [43]:\n",
    "In [9]:\n",
    "In [10]:\n",
    "In [11]:\n",
    "Out[8]:\n",
    "'Hello! Welcome to NLP Learning. This is the day 1 of Webinar. We are trying \n",
    "to understand text basics. Lets move ahead.'\n",
    "I \n",
    "a red dress \n",
    "doc1.text\n",
    "doc1 = nlp(u'I have a red dress') \n",
    "for nc in doc1.noun_chunks:\n",
    "    print(nc)\n",
    "# ...............................Step 2: Tokenization......................................\n",
    "# During processing, spaCy first tokenizes the text & segments it into words, punctuation a\n",
    "# This is done by applying rules specific to each language. \n",
    "# For example:\n",
    "# Punctuation at the end of a sentence should be split off – whereas “U.K.” should remain o\n",
    "# Each Doc consists of individual tokens, and we can iterate over them.\n",
    "#................................A. Part-of-speech tags....................................\n",
    "# Visit: https://spacy.io/api/annotation#pos-tagging\n",
    "# 1: Create a Doc object for a Unicode string as under:\n",
    "doc = nlp(u'India has huge surge of I.T. jobs with business worth of $20 billion. Welcome A\n",
    "\n",
    "\n",
    "In [12]:\n",
    "India   PROPN   nsubj   noun, proper singular   nominal subject \n",
    "has  \n",
    " AUX  \n",
    " ROOT   verb, 3rd person singular present  \n",
    " None \n",
    "huge  \n",
    " ADJ  \n",
    " amod   adjective  \n",
    " adjectival modifier \n",
    "surge   NOUN   dobj   noun, singular or mass  \n",
    " direct object \n",
    "of  \n",
    " ADP  \n",
    " prep   conjunction, subordinating or preposition  \n",
    " pre\n",
    "positional modifier \n",
    "I.T.  \n",
    " PROPN   compound  \n",
    " noun, proper singular   compound \n",
    "jobs  \n",
    " NOUN   pobj   noun, plural   object of preposition \n",
    "with  \n",
    " ADP  \n",
    " prep   conjunction, subordinating or preposition  \n",
    " pre\n",
    "positional modifier \n",
    "business  \n",
    " NOUN   compound  \n",
    " noun, singular or mass  \n",
    " com\n",
    "pound \n",
    "worth   ADJ  \n",
    " pobj   adjective  \n",
    " object of preposition \n",
    "of  \n",
    " ADP  \n",
    " prep   conjunction, subordinating or preposition  \n",
    " pre\n",
    "positional modifier \n",
    "$  \n",
    " SYM  \n",
    " quantmod  \n",
    " symbol, currency  \n",
    " modifier of quantif\n",
    "ier \n",
    "20  \n",
    " NUM  \n",
    " compound  \n",
    " cardinal number  \n",
    " compound \n",
    "billion  \n",
    " NUM  \n",
    " pobj   cardinal number  \n",
    " object of prepositi\n",
    "on \n",
    ".  \n",
    " PUNCT   punct   punctuation mark, sentence closer  \n",
    " punctuation \n",
    "Welcome  \n",
    " VERB   ROOT   verb, base form  \n",
    " None \n",
    "All  \n",
    " DET  \n",
    " dobj   determiner  \n",
    " direct object \n",
    "!  \n",
    " PUNCT   punct   punctuation mark, sentence closer  \n",
    " punctuation \n",
    "# 2: Now we will try to print each token along with some token attributes: \n",
    "for token in doc:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.dep_, '\\t', spacy.explain(token.tag_),'\n",
    "\n",
    "\n",
    "In [13]:\n",
    "In [14]:\n",
    "India          PROPN     nsubj     noun, proper singular                     \n",
    "nominal subject \n",
    "has            AUX       ROOT      verb, 3rd person singular present         \n",
    "None \n",
    "huge           ADJ       amod      adjective                                 \n",
    "adjectival modifier \n",
    "surge          NOUN      dobj      noun, singular or mass                    \n",
    "direct object \n",
    "of             ADP       prep      conjunction, subordinating or preposition \n",
    "prepositional modifier \n",
    "I.T.           PROPN     compound  noun, proper singular                     \n",
    "compound \n",
    "jobs           NOUN      pobj      noun, plural                              \n",
    "object of preposition \n",
    "with           ADP       prep      conjunction, subordinating or preposition \n",
    "prepositional modifier \n",
    "business       NOUN      compound  noun, singular or mass                    \n",
    "compound \n",
    "worth          ADJ       pobj      adjective                                 \n",
    "object of preposition \n",
    "of             ADP       prep      conjunction, subordinating or preposition \n",
    "prepositional modifier \n",
    "$              SYM       quantmod  symbol, currency                          \n",
    "modifier of quantifier \n",
    "20             NUM       compound  cardinal number                           \n",
    "compound \n",
    "billion        NUM       pobj      cardinal number                           \n",
    "object of preposition \n",
    ".              PUNCT     punct     punctuation mark, sentence closer         \n",
    "punctuation \n",
    "Welcome        VERB      ROOT      verb, base form                           \n",
    "None \n",
    "All            DET       dobj      determiner                                \n",
    "direct object \n",
    "!              PUNCT     punct     punctuation mark, sentence closer         \n",
    "punctuation \n",
    "has \n",
    "have \n",
    "# Step 3: Now we will try to print each token along with some token attributes: \n",
    "for token in doc:\n",
    "    print(f'{token.text:{15}}{token.pos_:{10}}{token.dep_:{10}}{spacy.explain(token.tag_):{\n",
    "# Demo of Lemmas (the base form of the word):\n",
    "print(doc[1].text)\n",
    "print(doc[1].lemma_)\n",
    "\n",
    "\n",
    "In [15]:\n",
    "In [16]:\n",
    "In [17]:\n",
    "In [18]:\n",
    "India => Xxxxx \n",
    "I.T. => X.X. \n",
    "True \n",
    "False \n",
    "18 \n",
    "True \n",
    "True \n",
    "# Demo of Word text and word Shapes:\n",
    "print(doc[0].text +' => ' + doc[0].shape_)\n",
    "print(doc[5].text +' => ' + doc[5].shape_)\n",
    "# Demo 1 of check for alphabets and stop words:\n",
    "print(doc[0].is_alpha)\n",
    "print(doc[0].is_stop)\n",
    "print(len(doc))\n",
    "# Demo 2 of check for alphabets and stop words:\n",
    "print(doc[1].is_alpha)\n",
    "print(doc[1].is_stop)\n",
    "#..................................B. Dependecy Parsing....................................\n",
    "\n",
    "\n",
    "In [46]:\n",
    "Suman \n",
    "PROPN\n",
    "is \n",
    "AUX\n",
    "learning \n",
    "VERB\n",
    "python \n",
    "PROPN\n",
    "n\n",
    "s\n",
    "u\n",
    "b\n",
    "j\n",
    "a\n",
    "u\n",
    "x\n",
    "d\n",
    "o\n",
    "b\n",
    "j\n",
    "from spacy import displacy\n",
    "doct4 = nlp(u'Suman is learning python')\n",
    "displacy.render(doct4, style='dep', jupyter=True, options={'distance': 90})\n",
    "# Here the sentence contains three relationships:\n",
    "# 1. nsubj is the subject of the word. Its headword is a verb.\n",
    "# 2. aux is an auxiliary word. Its headword is a verb.\n",
    "# 3. dobj is the direct object of the verb. Its headword is a verb.\n",
    "# 4. style: dep => dependency, ent => entities\n",
    "\n",
    "\n",
    "In [20]:\n",
    "# Demo of parse tree and its properties\n",
    "from spacy import displacy\n",
    "doc11 = nlp(u'Suman is learning python from Squad')\n",
    "displacy.render(doc11, style='dep', jupyter=True, options={'distance': 90})\n",
    "# Extract all tokens on the left of 'learning'\n",
    "print(\"Token on left of \\'learning\\': \", [token.text for token in doc11[2].lefts])\n",
    "# Extract all tokens on the right of 'learning'\n",
    "print(\"Token on right of \\'learning\\': \",[token.text for token in doc11[2].rights])\n",
    "# Extract all children tokens of 'learning'\n",
    "print(\"Children Tokens of \\'learning\\': \",[token.text for token in doc11[2].children])\n",
    "# Extract neighbour tokens following or on right side of 'learning'\n",
    "print(\"The neigbour token on right of \\'learning\\': \", doc11[2].nbor()) \n",
    "# Extract neighbour tokens preceeding or on left of 'learning'\n",
    "print(\"The neigbour token on left of \\'learning\\': \", doc11[2].nbor(-1)) \n",
    "# Display number of token on left and right of 'learning'\n",
    "print(\"The number of token on left of \\'learning\\': \", doc11[2].n_lefts)  \n",
    "print(\"The number of token on right of \\'learning\\': \", doc11[2].n_rights)\n",
    "# Display subtree of 'learning'\n",
    "print(\"Subtree of \\'learning\\': \", list(doc11[2].subtree))\n",
    "print(\"-----------------------------------------------------------\")\n",
    "# Extract all tokens on the left of 'from'\n",
    "print(\"Token on left of \\'from\\': \", [token.text for token in doc11[4].lefts])\n",
    "# Extract all tokens on the right of 'from'\n",
    "print(\"Token on right of \\'from\\': \",[token.text for token in doc11[4].rights])\n",
    "# Extract all children tokens of 'from'\n",
    "print(\"Children Tokens of \\'from\\': \",[token.text for token in doc11[4].children])\n",
    "# Extract neighbour tokens following or on right side of 'from'\n",
    "print(\"The neigbour token on right of \\'from\\': \", doc11[4].nbor()) \n",
    "# Extract neighbour tokens preceeding or on left of 'from'\n",
    "print(\"The neigbour token on left of \\'from\\': \", doc11[4].nbor(-1)) \n",
    "# Display number of token on left and right of 'from'\n",
    "print(\"The number of token on left of \\'from\\': \", doc11[4].n_lefts)  \n",
    "print(\"The number of token on right of \\'from\\': \", doc11[4].n_rights)\n",
    "# Display subtree of 'from'\n",
    "print(\"Subtree of \\'from\\': \", list(doc11[4].subtree))\n",
    "\n",
    "\n",
    "In [21]:\n",
    "In [47]:\n",
    "Token on left of 'learning':  ['Suman', 'is'] \n",
    "Token on right of 'learning':  ['python', 'from'] \n",
    "Children Tokens of 'learning':  ['Suman', 'is', 'python', 'from'] \n",
    "The neigbour token on right of 'learning':  python \n",
    "The neigbour token on left of 'learning':  is \n",
    "The number of token on left of 'learning':  2 \n",
    "The number of token on right of 'learning':  2 \n",
    "Subtree of 'learning':  [Suman, is, learning, python, from, Squad] \n",
    "----------------------------------------------------------- \n",
    "Token on left of 'from':  [] \n",
    "Token on right of 'from':  ['Squad'] \n",
    "Children Tokens of 'from':  ['Squad'] \n",
    "The neigbour token on right of 'from':  Squad \n",
    "The neigbour token on left of 'from':  python \n",
    "The number of token on left of 'from':  0 \n",
    "The number of token on right of 'from':  1 \n",
    "Subtree of 'from':  [from, Squad] \n",
    "Suman \n",
    "PROPN\n",
    "is \n",
    "AUX\n",
    "learning \n",
    "VERB\n",
    "python \n",
    "NOUN\n",
    "from \n",
    "ADP\n",
    "Squad \n",
    "PROPN\n",
    "n\n",
    "s\n",
    "u\n",
    "b\n",
    "j\n",
    "a\n",
    "u\n",
    "x\n",
    "d\n",
    "o\n",
    "b\n",
    "j\n",
    "p\n",
    "r\n",
    "e\n",
    "p\n",
    "p\n",
    "o\n",
    "b\n",
    "j\n",
    "\"Hello! We're learning Natural Language Processing in I.T. basics!. It's eas\n",
    "y don't worry.\" \n",
    "# Functionality of Tokenization:\n",
    "# 1. First, the raw text is split on whitespace characters, similar to text.split(' '). \n",
    "# 2. Then, the tokenizer processes the text from left to right. On each substring, it perfo\n",
    "#    two checks:\n",
    "#    a. Does the substring match a tokenizer exception rule? \n",
    "#       For example, “don’t” does not contain whitespace, but should be split into two toke\n",
    "#       “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "#    b. Can a prefix, suffix or infix be split off? \n",
    "#       For example punctuation like commas, periods, hyphens or quotes.\n",
    "# Creating a new string named string1\n",
    "string1 = '\"Hello! We\\'re learning Natural Language Processing in I.T. basics!. It\\'s easy \n",
    "print(string1)\n",
    "\n",
    "\n",
    "In [49]:\n",
    "In [24]:\n",
    "In [25]:\n",
    "In [26]:\n",
    "In [27]:\n",
    "\" || Hello || ! || We || 're || learning || Natural || Language || Processin\n",
    "g || in || I.T. || basics || ! || . || It || 's || easy || do || n't || worr\n",
    "y || . || \" || \n",
    "Out[24]:\n",
    "22\n",
    "Out[25]:\n",
    "! We're\n",
    "Out[26]:\n",
    "n't worry.\"\n",
    "--------------------------------------------------------------------------- \n",
    "TypeError                                 Traceback (most recent call last) \n",
    "<ipython-input-27-7541e1c25e19> in <module> \n",
    "      1 # Note: Item assignment not allowed!!!! \n",
    "      2  \n",
    "----> 3 doc2[2]=doc2[4] \n",
    " \n",
    "TypeError: 'spacy.tokens.doc.Doc' object does not support item assignment \n",
    " \n",
    "# Creating a Doc2 object and exploring all tokens in string 1\n",
    "doc2 = nlp(string1)\n",
    "for token in doc2:\n",
    "    print(token.text, end=' || ')\n",
    "# To count number of tokens\n",
    "len(doc2)\n",
    "# Demo to fetch three tokens from the middle of string\n",
    "doc2[2:5]\n",
    "# Demo to fetch the last four tokens of the string\n",
    "doc2[-4:]\n",
    "# Note: Item assignment not allowed!!!!\n",
    "doc2[2]=doc2[4]\n",
    "\n",
    "\n",
    "In [25]:\n",
    "In [29]:\n",
    "In [30]:\n",
    "In [31]:\n",
    "In [32]:\n",
    "Google || is || setting || up || $ || 50 || million || company || branch || \n",
    "in || Delhi ||  \n",
    " \n",
    "Google - ORG - Companies, agencies, institutions, etc. \n",
    "$50 million - MONEY - Monetary values, including unit \n",
    "Delhi - GPE - Countries, cities, states \n",
    "Out[30]:\n",
    "3\n",
    "Google ORG \n",
    "is setting up \n",
    "$50 million MONEY \n",
    "company branch in \n",
    "Delhi GPE\n",
    "# ..............................Step 3. Named Entity Recognition...........................\n",
    "# View Named Entities\n",
    "# A named entity is a “real-world object” that’s assigned a name – for example,\n",
    "# a person, a country, a product or a book title.\n",
    "doc3 = nlp(u'Google is setting up $50 million company branch in Delhi')\n",
    "for token in doc3:\n",
    "    print(token.text, end=' || ')\n",
    "print('\\n')\n",
    "# Named entities are available as the .ents property of a Doc.\n",
    "for entity in doc3.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))\n",
    "# count number of named entites\n",
    "len(doc3.ents)\n",
    "# Using spaCy’s built-in displaCy visualizer, the sentence and its named entities can be di\n",
    "# spaCy includes a built-in visualization tool called displaCy.\n",
    "# Demo to view all entities in Displacy\n",
    "from spacy import displacy\n",
    "doc4 = nlp(u'Google is setting up $50 million company branch in Delhi')\n",
    "displacy.render(doc4, style='ent', jupyter=True)\n",
    "\n",
    "\n",
    "In [ ]:\n",
    "In [34]:\n",
    "In [ ]:\n",
    "In [36]:\n",
    "In [ ]:\n",
    "dog dog 1.0 \n",
    "dog cat 0.80168545 \n",
    "dog banana 0.24327648 \n",
    "cat dog 0.80168545 \n",
    "cat cat 1.0 \n",
    "cat banana 0.28154367 \n",
    "banana dog 0.24327648 \n",
    "banana cat 0.28154367 \n",
    "banana banana 1.0 \n",
    "# spaCy is able to compare two objects, and make a prediction of how similar they are. \n",
    "# Predicting similarity is useful for building recommendation systems or flagging duplicate\n",
    "# For example, you can suggest a user content that’s similar to what they’re currently look\n",
    "# or label a support ticket as a duplicate if it’s very similar to an already existing one.\n",
    "# Each Doc, Span and Token comes with a .similarity() method that lets you compare it with \n",
    "# object, and determine the similarity.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\n",
    "tokens = nlp(\"dog cat banana\")\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "# In the above case, the model’s predictions are pretty on point. \n",
    "# A dog is very similar to a cat, whereas a banana is not very similar to either of them. \n",
    "# Identical tokens are obviously 100% similar to each other \n",
    "# (just not always exactly 1.0, because of vector math and floating point imprecisions.\n",
    "#.....................................Step 4: Stemming Demo................................\n",
    "# Stemming is the process of producing morphological variants of a root/base word. \n",
    "# Stemming programs are commonly referred to as stemming algorithms or stemmers. \n",
    "# Stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to root word, “c\n",
    "# and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”.\n",
    "# One of the most popular stemming algorithms is the Porter stemmer, which has been around \n",
    "\n",
    "\n",
    "In [37]:\n",
    "In [38]:\n",
    "In [39]:\n",
    "In [40]:\n",
    "In [1]:\n",
    "look ==> look \n",
    "looks ==> look \n",
    "looked ==> look \n",
    "looking ==> look \n",
    "play ==> play \n",
    "player ==> player \n",
    "playing ==> play \n",
    "run ==> run \n",
    "ran ==> ran \n",
    "runs ==> run \n",
    "program  :  program \n",
    "programs  :  program \n",
    "programer  :  program \n",
    "programing  :  program \n",
    "programers  :  program \n",
    "# Import the nltk toolkit and Porter Stemmer library\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "# First, we're going to grab and define our stemmer:\n",
    "stem1 = PorterStemmer()\n",
    "# Now, let's choose some testwords with a similar stem, like:\n",
    "testwords = ['look','looks','looked','looking','play','player','playing','run','ran','runs'\n",
    "for tw in testwords:\n",
    "    print(tw+' ==> '+ stem1.stem(tw))\n",
    "# import these modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) \n",
    "\n",
    "\n",
    "In [5]:\n",
    "In [41]:\n",
    "In [42]:\n",
    "It \n",
    "is \n",
    "import \n",
    "to \n",
    "be \n",
    "veri \n",
    "pythonli \n",
    "while \n",
    "you \n",
    "are \n",
    "python \n",
    "with \n",
    "python. \n",
    "all \n",
    "python \n",
    "have \n",
    "python \n",
    "poorli \n",
    "at \n",
    "least \n",
    "onc \n",
    ". \n",
    "# Now let's try stemming a typical sentence, rather than some words:\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "new_text = \"It is important to be very pythonly while you are pythoning with python. All py\n",
    "words = word_tokenize(new_text, preserve_line = \"true\")\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n",
    "#...................................Step 5: Lemmatization Demo.............................\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "In [45]:\n",
    "I  \n",
    " PRON   -PRON- \n",
    "like  \n",
    " VERB   like \n",
    "watching  \n",
    " VERB   watch \n",
    "cricket  \n",
    " NOUN   cricket \n",
    "matches  \n",
    " NOUN   match \n",
    ".  \n",
    " PUNCT   . \n",
    "The  \n",
    " DET  \n",
    " the \n",
    "best  \n",
    " ADJ  \n",
    " good \n",
    "players  \n",
    " NOUN   player \n",
    "are  \n",
    " AUX  \n",
    " be \n",
    "sachin   ADJ  \n",
    " sachin \n",
    "and  \n",
    " CCONJ   and \n",
    "virat   NOUN   virat \n",
    ",  \n",
    " PUNCT   , \n",
    "one  \n",
    " NUM  \n",
    " one \n",
    "played   VERB   play \n",
    "for  \n",
    " ADP  \n",
    " for \n",
    "India   PROPN   India \n",
    "other   ADJ  \n",
    " other \n",
    "still   ADV  \n",
    " still \n",
    "playing  \n",
    " VERB   play \n",
    "doc6 = nlp(u\"I like watching cricket matches. The best players are sachin and virat, one pl\n",
    "for tk in doc6:\n",
    "    print(tk.text, '\\t', tk.pos_, '\\t', tk.lemma_)\n",
    "\n",
    "\n",
    "In [46]:\n",
    "In [47]:\n",
    "In [6]:\n",
    "I                    PRON       -PRON- \n",
    "like                 VERB       like \n",
    "watching             VERB       watch \n",
    "cricket              NOUN       cricket \n",
    "matches              NOUN       match \n",
    ".                    PUNCT      . \n",
    "The                  DET        the \n",
    "best                 ADJ        good \n",
    "players              NOUN       player \n",
    "are                  AUX        be \n",
    "sachin               ADJ        sachin \n",
    "and                  CCONJ      and \n",
    "virat                NOUN       virat \n",
    ",                    PUNCT      , \n",
    "one                  NUM        one \n",
    "played               VERB       play \n",
    "for                  ADP        for \n",
    "India                PROPN      India \n",
    "other                ADJ        other \n",
    "still                ADV        still \n",
    "playing              VERB       play \n",
    "# Formatting with F-String\n",
    "doc6 = nlp(u\"I like watching cricket matches. The best players are sachin and virat, one pl\n",
    "for tk in doc6:\n",
    "    print(f'{tk.text:{20}} {tk.pos_:{10}} {tk.lemma_}')\n",
    "#..................................Step 6: Demo of Stop words..............................\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "In [7]:\n",
    "In [8]:\n",
    "{'former', 'hereafter', 'hers', 'may', \"'s\", 'what', 'only', 'beside', 'al\n",
    "l', 'any', 'can', 'whereupon', 'around', 'due', 'somehow', 'which', 'third', \n",
    "'it', 'six', 'amongst', 'just', 'sixty', 'therein', 'twenty', 'empty', 'shou\n",
    "ld', 'therefore', 'back', 'as', 'into', 'nobody', 'for', 'these', 'throughou\n",
    "t', 'nowhere', 'own', 'whence', 'whenever', 'would', 'hundred', 'serious', \n",
    "'was', 'across', 'none', 'side', 'why', 'is', 'see', 'both', 'above', 'belo\n",
    "w', 'mostly', '’m', 'each', 'toward', 'go', 'through', \"'ve\", '’ll', '’ve', \n",
    "'everything', 'hence', 'via', 'already', 'themselves', 'him', 'whatever', 'c\n",
    "a', 'the', 'another', 'everywhere', 'noone', 'n‘t', 'they', 'i', 'them', 'bu\n",
    "t', 'last', 'seems', 'eight', 'fifteen', 'most', 'thereupon', 'somewhere', \n",
    "'out', 'full', 'else', 'nine', 'me', 'moreover', 'quite', 'us', 're', 'where\n",
    "by', 'whereafter', 'have', 'more', 'here', 'will', 'show', 'after', 'when', \n",
    "'forty', 'even', 'did', 'indeed', 'elsewhere', 'anywhere', 'ours', 'during', \n",
    "'same', 'over', 'whose', 'thence', 'then', 'by', 'per', 'enough', 'less', 's\n",
    "uch', 'now', 'next', 'herein', 'always', 'nothing', 'or', 'become', 'sometim\n",
    "e', 'either', 'although', 'his', 'least', 'her', 'my', 'nevertheless', 'bee\n",
    "n', 'put', 'how', 'there', 'must', 'something', 'yourself', 'be', 'along', \n",
    "'several', 'from', 'so', 'its', 'hereupon', 'of', 'might', 'however', 'latte\n",
    "r', '‘s', 'eleven', 'if', 'she', 'five', 'say', 'thereby', '‘d', 'other', 'a\n",
    "nyone', 'afterwards', 'really', 'within', '’s', 'behind', 'latterly', 'anywa\n",
    "y', 'their', 'everyone', 'done', 'again', 'please', 'nor', 'made', 'anyhow', \n",
    "'four', 'well', \"'d\", 'though', 'down', 'keep', '‘re', '’d', 'between', 'see\n",
    "ming', 'without', 'anything', 'yet', 'bottom', 'whom', 'except', 'ourselve\n",
    "s', 'not', 'in', \"'re\", 'our', 'becomes', 'fifty', 'a', 'make', 'namely', 'o\n",
    "n', 'n’t', 'perhaps', 'towards', 'front', 'under', 'because', 'two', 'wherea\n",
    "s', 'also', 'name', 'no', 'to', 'seemed', 'where', 'among', 'using', 'does', \n",
    "'were', 'your', 'every', 'are', 'move', 'beyond', 'itself', 'we', 'one', 'si\n",
    "nce', 'off', 'too', 'thus', 'beforehand', 'otherwise', 'mine', 'upon', 'toge\n",
    "ther', 'wherein', 'onto', '‘m', 'ever', 'rather', 'who', 'others', 'furthe\n",
    "r', 'this', 'had', 'himself', 'against', 'someone', 'ten', 'doing', 'do', 'w\n",
    "herever', 'never', \"n't\", 'those', \"'ll\", 'first', 'some', 'top', 'yours', \n",
    "'get', 'take', 'whoever', 'about', 'many', 'myself', 'alone', 'call', 'canno\n",
    "t', 'thru', 'amount', 'neither', 'twelve', 'yourselves', \"'m\", 'seem', 'ther\n",
    "eafter', 'became', '‘ll', 'still', 'much', 'he', 'herself', 'before', 'has', \n",
    "'that', 'unless', 'and', 'whither', 'few', 'meanwhile', 'part', 'formerly', \n",
    "'until', 'whole', 'various', 'could', 'being', 'often', '‘ve', 'very', 'here\n",
    "by', 'sometimes', 'up', 'once', 'used', '’re', 'with', 'regarding', 'you', \n",
    "'an', 'at', 'than', 'almost', 'becoming', 'whether', 'give', 'while', 'am', \n",
    "'besides', 'three'} \n",
    "Out[8]:\n",
    "326\n",
    "# Display the set of spaCy's list of default stop words.\n",
    "print(nlp.Defaults.stop_words)\n",
    "len(nlp.Defaults.stop_words)\n",
    "\n",
    "\n",
    "In [9]:\n",
    "In [10]:\n",
    "In [11]:\n",
    "In [12]:\n",
    "In [13]:\n",
    "In [14]:\n",
    "In [15]:\n",
    "Out[9]:\n",
    "True\n",
    "Out[10]:\n",
    "False\n",
    "Out[12]:\n",
    "327\n",
    "Out[13]:\n",
    "True\n",
    "Out[15]:\n",
    "326\n",
    "# Chaeck if a word is a stop word or not\n",
    "nlp.vocab['call'].is_stop\n",
    "nlp.vocab['calling'].is_stop\n",
    "# Let us add the word \"reality\" to the set of stop words\n",
    "nlp.Defaults.stop_words.add('calling')\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['calling'].is_stop = True\n",
    "len(nlp.Defaults.stop_words)\n",
    "nlp.vocab['calling'].is_stop\n",
    "# Remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('calling')\n",
    "# Remove the stop_word tag from the set\n",
    "nlp.vocab['calling'].is_stop = False\n",
    "len(nlp.Defaults.stop_words)\n",
    "\n",
    "\n",
    "In [67]:\n",
    "In [49]:\n",
    "In [1]:\n",
    "In [2]:\n",
    "The brave girl \n",
    "a small rabbit \n",
    "a wild fox \n",
    "dense forest \n",
    "#....................Demo of Noun Chunks.......................\n",
    "doc7 = nlp(u'The brave girl saved a small rabbit from a wild fox running in dense forest')\n",
    "for ck in doc7.noun_chunks:\n",
    "    print(ck.text)\n",
    "#.............................Rule Based Pattern matching Demo........................\n",
    "# spaCy enables using rule-matching with a tool called Matcher that allows you to build a l\n",
    "# of token patterns.\n",
    "# These patterns are then matched with a Doc object and finally returns a list of found mat\n",
    "# spaCy features a rule-matching engine, the Matcher, that operates over tokens, similar to\n",
    "# The rules can refer to token annotations (e.g. the token text or tag_, and flags (e.g. IS\n",
    "# Example:\n",
    "# Let’s say we want to enable spaCy to find a combination of three tokens:\n",
    "# 1. A token whose lowercase form matches “hello”, e.g. “Hello” or “HELLO”.\n",
    "# 2. A token whose is_punct flag is set to True, i.e. any punctuation Ex- hello-.\n",
    "# 3. A token whose lowercase form matches “world”, e.g. “World” or “WORLD”.\n",
    "#          [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "# When writing patterns, keep in mind that each dictionary represents one token.\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "In [3]:\n",
    "In [4]:\n",
    "In [5]:\n",
    "In [7]:\n",
    "In [10]:\n",
    "[(14870614998449374032, 4, 5), (14870614998449374032, 9, 12), (1487061499844\n",
    "9374032, 13, 15)] \n",
    "14870614998449374032 GoodLooking 4 5 goodlooking \n",
    "14870614998449374032 GoodLooking 9 12 good-looking \n",
    "14870614998449374032 GoodLooking 13 15 good looking \n",
    "# Importing the Matcher library from spacy\n",
    "# Here matcher is an object that links to the Vocab object. \n",
    "# First, we initialize the Matcher with a vocab. \n",
    "# The matcher must always share the same vocab with the documents it will operate on. \n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Creating Patterns to match.\n",
    "# We can add or remove specific named patterns to matcher as needed.\n",
    "# Note: single spaces are not tokenized, therefore dont count as punctuation.\n",
    "pattern1 = [{'LOWER': 'goodlooking'}]\n",
    "pattern2 = [{'LOWER': 'good'}, {'LOWER': 'looking'}]\n",
    "pattern3 = [{'LOWER': 'good'}, {'IS_PUNCT': True}, {'LOWER': 'looking'}]\n",
    "# We can now call matcher.add() with an ID and our custom pattern.\n",
    "# The second argument lets you pass in an optional callback function to invoke on a success\n",
    "# For now, we set it to None.\n",
    "matcher.add('GoodLooking', None, pattern1, pattern2, pattern3)\n",
    "doc55 = nlp(u'The best feature of goodlooking person is not just good-looking but good look\n",
    "# The matcher returns a list of (match_id, start, end) tuples\n",
    "# Each tuple contains an match_id, with start & end tokens in doc55[start:end]\n",
    "# Here match_id is simply the hash value of the string_ID 'GoodLooking'\n",
    "match_find = matcher(doc55)\n",
    "print(match_find)\n",
    "for match_id, start, end in match_find:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc55[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)\n",
    "\n",
    "\n",
    "In [ ]:\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
